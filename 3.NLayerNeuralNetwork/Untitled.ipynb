{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Layer():\n",
    "    def __init__(self,model, acti_func, d_acti_func,input_dim = None, output_dim = None, first_layer=False, last_layer=False,learning_rate = 0.0075):\n",
    "        self.model = model\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "\n",
    "        self.A = None\n",
    "        self.Z = None\n",
    "        \n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        self.dA = None # actually gradient of output of prev layer\n",
    "        self.dZ = None\n",
    "        \n",
    "        self.first_layer = first_layer\n",
    "        self.last_layer = last_layer\n",
    "        \n",
    "        self.acti_func = acti_func\n",
    "        self.d_acti_func = d_acti_func\n",
    "        \n",
    "        self.next_layer = None\n",
    "        self.prev_layer = None\n",
    "        \n",
    "    def random_initialize(self):\n",
    "        self.W = np.random.randn(self.output_dim, self.input_dim)/np.sqrt(self.input_dim)  #*np.sqrt(2/self.input_dim)\n",
    "        #self.W = np.random.randn(self.output_dim, self.input_dim)*0.01\n",
    "        self.b = np.zeros(shape=(self.output_dim, 1))\n",
    "        \n",
    "    def forward_propagate(self):\n",
    "        if self.first_layer:\n",
    "            prev_A = self.model.data\n",
    "        else:\n",
    "            prev_A = self.prev_layer.A\n",
    "\n",
    "        self.Z = self.W.dot(prev_A) + self.b\n",
    "                \n",
    "        self.A = self.acti_func(self.Z)\n",
    "        \n",
    "    def backward_propagate(self):\n",
    "        if self.first_layer:\n",
    "            prev_A = self.model.data\n",
    "        else:\n",
    "            prev_A = self.prev_layer.A\n",
    "            \n",
    "        if self.last_layer:\n",
    "            next_dA = self.model.calculate_cost_derivative(self.A)\n",
    "        else:\n",
    "            next_dA = self.next_layer.dA\n",
    "\n",
    "        m = prev_A.shape[1]\n",
    "                \n",
    "        self.dZ = next_dA*self.d_acti_func(self.Z)\n",
    "        self.dW = self.dZ.dot(prev_A.T)/m\n",
    "        self.db = np.sum(self.dZ, axis=1, keepdims=True)/m\n",
    "        self.dA = self.W.T.dot(self.dZ)\n",
    "        \n",
    "    def optimize(self):\n",
    "        self.W -= self.learning_rate*self.dW\n",
    "        self.b -= self.learning_rate*self.db\n",
    "\n",
    "\n",
    "#description = [{\"layer_size\" : 10, \"activation\" : \"sigmoid\"}, \n",
    "#               {\"layer_size\" : 20, \"activation\" : \"sigmoid\"}, \n",
    "#               {\"layer_size\" : 20, \"activation\" : \"sigmoid\"},\n",
    "#               {\"layer_size\" : 1, \"activation\" : \"sigmoid\"}]\n",
    "\n",
    "class NN():\n",
    "    def __init__(self, description, input_size, cost_function, train_data = None, train_labels = None,learning_rate = 0.0075):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.layers = self.create_architecture(description, input_size)\n",
    "        self.data = train_data\n",
    "        self.labels = train_labels\n",
    "        \n",
    "        # cost_function(y, y_hat)\n",
    "        self.cost_function, self.d_cost_function = cost_functions[cost_function]\n",
    "        \n",
    "    def calculate_cost(self, y_hat):\n",
    "        return self.cost_function(self.labels, y_hat)\n",
    "    \n",
    "    def calculate_cost_derivative(self, y_hat):\n",
    "        return self.d_cost_function(self.labels, y_hat)\n",
    "        \n",
    "    def calculate_accuracy(self,test_data, test_labels):\n",
    "        # Works for binary input right now\n",
    "        self.data = test_data\n",
    "        self.labels = test_labels\n",
    "        \n",
    "        self.forward_pass()\n",
    "        \n",
    "        y_hat = self.layers[-1].A\n",
    "        \n",
    "        pred = np.where(y_hat > 0.5, 1, 0)\n",
    "            \n",
    "        return (pred == self.labels).mean()\n",
    "                \n",
    "        \n",
    "    def create_architecture(self, description, input_size):\n",
    "        layers = []\n",
    "        \n",
    "        for index, descr in enumerate(description):\n",
    "            print(index)\n",
    "            input_dim = input_size if index == 0 else layers[-1].output_dim\n",
    "            output_dim = descr[\"layer_size\"]\n",
    "            activ, d_activ = activation_functions[descr[\"activation\"]]\n",
    "            \n",
    "            layer = Layer(self, activ, d_activ,input_dim, output_dim,\n",
    "                         first_layer=(index ==  0 ), last_layer = (index == len(description) - 1),learning_rate = self.learning_rate)\n",
    "            \n",
    "            # set pointers\n",
    "            if index != 0:\n",
    "                layers[-1].next_layer = layer\n",
    "                layer.prev_layer = layers[-1]\n",
    "                \n",
    "            layers.append(layer)\n",
    "    \n",
    "        # \"layers\" is populated now. Initialize weights\n",
    "        for layer in layers:\n",
    "            layer.random_initialize()\n",
    "            \n",
    "        return layers\n",
    "    \n",
    "    def add_data(self, train_data, train_labels):\n",
    "        self.data = train_data\n",
    "        self.labels = train_labels\n",
    "        \n",
    "    def forward_pass(self):\n",
    "        for layer in self.layers:            \n",
    "            layer.forward_propagate()\n",
    "            \n",
    "    def backward_pass(self):\n",
    "        for layer in reversed(self.layers):\n",
    "            layer.backward_propagate()\n",
    "            \n",
    "    def optimize(self):\n",
    "        for layer in self.layers:\n",
    "            layer.optimize()\n",
    "    \n",
    "    def train(self, epocs):\n",
    "        history = []\n",
    "        \n",
    "        for i in range(epocs):\n",
    "            self.forward_pass()\n",
    "        \n",
    "            cost = self.calculate_cost(self.layers[-1].A)\n",
    "            history.append(cost)\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                print (\"Cost after iteration %i: %f\" %(i, cost))        \n",
    "            self.backward_pass()\n",
    "                        \n",
    "            self.optimize()\n",
    "\n",
    "        \n",
    "        # Training done. Return history\n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cross_entropy_sigmoid(y, y_hat):\n",
    "    m = y.shape[1]\n",
    "    #cost = np.sum(y*np.log(y_hat) + (1-y)*np.log(1 - y_hat)) / (-1*m)\n",
    "    cost = (1./m) * (-np.dot(y,np.log(y_hat).T) - np.dot(1-y, np.log(1-y_hat).T))\n",
    "    \n",
    "    # So that we have a real number at the end instead of a singleton; e.g. [[3]] => 3\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def cross_entropy_softmax(y, y_hat):\n",
    "    # y is a vector of dimension [1 x num_of_inputs]\n",
    "    # IT IS NOT ONE HOT VECTOR !!!\n",
    "        \n",
    "    num_inputs = y_hat.shape[1]\n",
    "    \n",
    "    # get the probabilities indexed by classes, y_hat\n",
    "    probs = y_hat[y.squeeze(), range(num_inputs)]\n",
    "\n",
    "    \n",
    "    log_probs = np.log(probs)\n",
    "    \n",
    "    cost = np.sum(log_probs)/(-1*num_inputs)\n",
    "    \n",
    "    # So that we have a real number at the end instead of a singleton; e.g. [[3]] => 3\n",
    "    cost = cost.squeeze()\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def cross_entropy_sigmoid_derivative(y, y_hat):\n",
    "    m = y.shape[1]\n",
    "    return (-(np.divide(y, y_hat) - np.divide(1 - y, 1 - y_hat)))\n",
    "\n",
    "def cross_entropy_softmax_derivative(y, y_hat):\n",
    "    # y is a vector of dimension [1 x num_of_inputs]\n",
    "    # IT IS NOT ONE HOT VECTOR !!!\n",
    "\n",
    "    num_inputs = y_hat.shape[1]\n",
    "    \n",
    "    d = np.zeros(y_hat.shape)\n",
    "    \n",
    "    d[y, range(num_inputs)] = 1/y_hat[y, range(num_inputs)]\n",
    "    \n",
    "    return d/num_inputs\n",
    "\n",
    "def mean_squared(y, y_hat):\n",
    "    return  np.sum((y - y_hat)**2 ).squeeze() / (y_hat.shape[1]*2)\n",
    "\n",
    "def d_mean_squared(y, y_hat):\n",
    "    return (y_hat - y)\n",
    "\n",
    "\n",
    "cost_functions = {\"cross_entropy_sigmoid\" : (cross_entropy_sigmoid, cross_entropy_sigmoid_derivative),\n",
    "                 \"cross_entropy_softmax\" : (cross_entropy_softmax, cross_entropy_softmax_derivative),\n",
    "                  \"mean_squared\" : (mean_squared, d_mean_squared)\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    s = 1/(1+np.exp(-x))\n",
    "    \n",
    "    return s\n",
    "\n",
    "def d_sigmoid(x):\n",
    "    s = 1/(1+np.exp(-x))\n",
    "    \n",
    "    return s*(1-s)\n",
    "\n",
    "def relu(x):\n",
    "    r = np.maximum(0,x)\n",
    "    \n",
    "    assert(x.shape == r.shape)\n",
    "    \n",
    "    return r\n",
    "\n",
    "def d_relu(x):\n",
    "    r = np.where(x > 0, 1, 0)\n",
    "    \n",
    "    assert(x.shape == r.shape)\n",
    "    \n",
    "    return r\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def d_tanh(x):\n",
    "    d = tanh(x)\n",
    "    return 1 - d*d\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def d_linear(x):\n",
    "    return np.ones(shape=x.shape)\n",
    "\n",
    "activation_functions = {\"sigmoid\" : (sigmoid, d_sigmoid) , \"relu\" : (relu, d_relu), \"tanh\" : (tanh, d_tanh), \"linear\" : (linear, d_linear)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer, load_digits, load_iris\n",
    "\n",
    "def split_dataset(X,y):\n",
    "    # size of X : [num_input x num_dimension]\n",
    "    train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2)\n",
    "    train_X = train_X.T\n",
    "    test_X = test_X.T\n",
    "    \n",
    "    \n",
    "    return train_X, train_y, test_X, test_y\n",
    "\n",
    "def normalize_data(train_X, test_X):\n",
    "    means = np.mean(train_X, axis = 1, keepdims=True)\n",
    "    std_dev = np.std(train_X, axis = 1, keepdims=True)\n",
    "\n",
    "    train_X = (train_X - means)/std_dev\n",
    "    test_X = (test_X - means)/std_dev\n",
    "    \n",
    "    return train_X, test_X, means, std_dev\n",
    "\n",
    "\n",
    "# From https://machinelearningmastery.com/generate-test-datasets-python-scikit-learn/\n",
    "from sklearn.datasets.samples_generator import make_blobs, make_moons, make_regression,make_s_curve, make_friedman1\n",
    "from matplotlib import pyplot\n",
    "from pandas import DataFrame\n",
    "# # generate 2d classification dataset\n",
    "# X_1,y_1 = make_moons(n_samples=1000)\n",
    "# y_1 = np.array(y_1).reshape((len(y_1),1))\n",
    "# #X, y = make_blobs(n_samples=1000, centers=2, n_features=2)\n",
    "\n",
    "# # scatter plot, dots colored by class value\n",
    "# df = DataFrame(dict(x=X_1[:,0], y=X_1[:,1], label=y_1.squeeze()))\n",
    "# colors = {0:'red', 1:'blue', 2:'green'}\n",
    "# fig, ax = pyplot.subplots()\n",
    "# grouped = df.groupby('label')\n",
    "# print(X_1.shape, y_1.shape)\n",
    "# for key, group in grouped:\n",
    "#     group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "# pyplot.show()\n",
    "\n",
    "res = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "X_2,y_2 = res\n",
    "y_2 = y_2.reshape((len(y_2), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30, 455), (1, 455), (30, 114), (1, 114))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X, train_y, test_X, test_y = split_dataset(X_2,y_2)\n",
    "\n",
    "train_y = train_y.reshape((1, len(train_y)))\n",
    "test_y = test_y.reshape((1, len(test_y)))\n",
    "\n",
    "# normalize_data\n",
    "train_X, test_X, means, std_dev = normalize_data(train_X, test_X)\n",
    "\n",
    "train_X.shape, train_y.shape, test_X.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "Cost after iteration 0: 0.722385\n",
      "Cost after iteration 100: 0.697372\n",
      "Cost after iteration 200: 0.677074\n",
      "Cost after iteration 300: 0.659544\n",
      "Cost after iteration 400: 0.643883\n",
      "Cost after iteration 500: 0.629288\n",
      "Cost after iteration 600: 0.615450\n",
      "Cost after iteration 700: 0.602025\n",
      "Cost after iteration 800: 0.589006\n",
      "Cost after iteration 900: 0.576344\n",
      "Cost after iteration 1000: 0.563781\n",
      "Cost after iteration 1100: 0.551335\n",
      "Cost after iteration 1200: 0.538830\n",
      "Cost after iteration 1300: 0.526214\n",
      "Cost after iteration 1400: 0.513492\n",
      "Cost after iteration 1500: 0.500589\n",
      "Cost after iteration 1600: 0.487802\n",
      "Cost after iteration 1700: 0.474926\n",
      "Cost after iteration 1800: 0.462064\n",
      "Cost after iteration 1900: 0.449341\n",
      "Cost after iteration 2000: 0.436702\n",
      "Cost after iteration 2100: 0.424042\n",
      "Cost after iteration 2200: 0.411385\n",
      "Cost after iteration 2300: 0.398745\n",
      "Cost after iteration 2400: 0.386120\n",
      "Cost after iteration 2500: 0.373476\n",
      "Cost after iteration 2600: 0.360829\n",
      "Cost after iteration 2700: 0.348214\n",
      "Cost after iteration 2800: 0.335621\n",
      "Cost after iteration 2900: 0.323117\n",
      "Cost after iteration 3000: 0.310746\n",
      "Cost after iteration 3100: 0.298548\n",
      "Cost after iteration 3200: 0.286604\n",
      "Cost after iteration 3300: 0.274966\n",
      "Cost after iteration 3400: 0.263672\n",
      "Cost after iteration 3500: 0.252782\n",
      "Cost after iteration 3600: 0.242321\n",
      "Cost after iteration 3700: 0.232335\n",
      "Cost after iteration 3800: 0.222878\n",
      "Cost after iteration 3900: 0.213934\n",
      "Cost after iteration 4000: 0.205513\n",
      "Cost after iteration 4100: 0.197615\n",
      "Cost after iteration 4200: 0.190218\n",
      "Cost after iteration 4300: 0.183311\n",
      "Cost after iteration 4400: 0.176860\n",
      "Cost after iteration 4500: 0.170828\n",
      "Cost after iteration 4600: 0.165199\n",
      "Cost after iteration 4700: 0.159949\n",
      "Cost after iteration 4800: 0.155059\n",
      "Cost after iteration 4900: 0.150497\n",
      "Cost after iteration 5000: 0.146223\n",
      "Cost after iteration 5100: 0.142209\n",
      "Cost after iteration 5200: 0.138443\n",
      "Cost after iteration 5300: 0.134893\n",
      "Cost after iteration 5400: 0.131533\n",
      "Cost after iteration 5500: 0.128357\n",
      "Cost after iteration 5600: 0.125353\n",
      "Cost after iteration 5700: 0.122514\n",
      "Cost after iteration 5800: 0.119824\n",
      "Cost after iteration 5900: 0.117283\n",
      "Cost after iteration 6000: 0.114884\n",
      "Cost after iteration 6100: 0.112612\n",
      "Cost after iteration 6200: 0.110455\n",
      "Cost after iteration 6300: 0.108405\n",
      "Cost after iteration 6400: 0.106452\n",
      "Cost after iteration 6500: 0.104591\n",
      "Cost after iteration 6600: 0.102818\n",
      "Cost after iteration 6700: 0.101125\n",
      "Cost after iteration 6800: 0.099509\n",
      "Cost after iteration 6900: 0.097965\n",
      "Cost after iteration 7000: 0.096490\n",
      "Cost after iteration 7100: 0.095080\n",
      "Cost after iteration 7200: 0.093731\n",
      "Cost after iteration 7300: 0.092442\n",
      "Cost after iteration 7400: 0.091205\n",
      "Cost after iteration 7500: 0.090018\n",
      "Cost after iteration 7600: 0.088878\n",
      "Cost after iteration 7700: 0.087783\n",
      "Cost after iteration 7800: 0.086729\n",
      "Cost after iteration 7900: 0.085720\n",
      "Cost after iteration 8000: 0.084747\n",
      "Cost after iteration 8100: 0.083806\n",
      "Cost after iteration 8200: 0.082896\n",
      "Cost after iteration 8300: 0.082015\n",
      "Cost after iteration 8400: 0.081163\n",
      "Cost after iteration 8500: 0.080340\n",
      "Cost after iteration 8600: 0.079543\n",
      "Cost after iteration 8700: 0.078772\n",
      "Cost after iteration 8800: 0.078024\n",
      "Cost after iteration 8900: 0.077298\n",
      "Cost after iteration 9000: 0.076592\n",
      "Cost after iteration 9100: 0.075905\n",
      "Cost after iteration 9200: 0.075238\n",
      "Cost after iteration 9300: 0.074588\n",
      "Cost after iteration 9400: 0.073955\n",
      "Cost after iteration 9500: 0.073340\n",
      "Cost after iteration 9600: 0.072741\n",
      "Cost after iteration 9700: 0.072157\n",
      "Cost after iteration 9800: 0.071585\n",
      "Cost after iteration 9900: 0.071027\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f08c571bd30>]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "description = [{\"layer_size\" : 100, \"activation\" : \"relu\"},\n",
    "               {\"layer_size\" : 50, \"activation\" : \"relu\"},\n",
    "               {\"layer_size\" : 10, \"activation\" : \"relu\"},\n",
    "               {\"layer_size\" : 1, \"activation\" : \"sigmoid\"}]\n",
    "\n",
    "model = NN(description,30,\"cross_entropy_sigmoid\", train_X, train_y, learning_rate=0.001)\n",
    "\n",
    "history = model.train(10000)\n",
    "\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the training set is = 0.6916666666666667\n",
      "Accuracy of the model on the test set is = 0.5666666666666667\n"
     ]
    }
   ],
   "source": [
    "acc = model.calculate_accuracy(train_X, train_y)\n",
    "print(\"Accuracy of the model on the training set is = {}\".format(acc))\n",
    "\n",
    "acc = model.calculate_accuracy(test_X, test_y)\n",
    "print(\"Accuracy of the model on the test set is = {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
